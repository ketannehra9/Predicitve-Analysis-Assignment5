{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8mwUxrIiRCOscROgm4ZzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketannehra9/Predicitve-Analysis-Assignment5/blob/main/notebook/Copy_of_PAS_Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TOPSIS to find best pre-trained model for Text Generation\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q torch transformers evaluate rouge_score sacrebleu nltk pandas matplotlib seaborn\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import evaluate\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# dataset (generated by chatgpt)\n",
        "data = [\n",
        "    {\"prompt\": \"The city experienced severe traffic congestion because\",\n",
        "     \"reference\": \"multiple roads were closed for construction work.\"},\n",
        "\n",
        "    {\"prompt\": \"Remote work became popular during the pandemic since\",\n",
        "     \"reference\": \"employees were required to work from home.\"},\n",
        "\n",
        "    {\"prompt\": \"Cybersecurity threats continue to rise as\",\n",
        "     \"reference\": \"more systems become connected to the internet.\"},\n",
        "\n",
        "    {\"prompt\": \"A strong team performs well when\",\n",
        "     \"reference\": \"members communicate openly and collaborate effectively.\"},\n",
        "\n",
        "    {\"prompt\": \"Online education platforms are useful because\",\n",
        "     \"reference\": \"they allow students to learn from anywhere.\"},\n",
        "\n",
        "    {\"prompt\": \"Economic inflation affects consumers when\",\n",
        "     \"reference\": \"prices increase faster than income.\"},\n",
        "\n",
        "    {\"prompt\": \"Climate change mitigation is difficult because\",\n",
        "     \"reference\": \"it requires cooperation across multiple countries.\"},\n",
        "\n",
        "    {\"prompt\": \"Artificial intelligence is being adopted rapidly as\",\n",
        "     \"reference\": \"businesses seek automation and efficiency.\"},\n",
        "\n",
        "    {\"prompt\": \"Data privacy concerns arise when\",\n",
        "     \"reference\": \"personal information is collected without consent.\"},\n",
        "\n",
        "    {\"prompt\": \"Social media influences public opinion by\",\n",
        "     \"reference\": \"spreading information quickly to large audiences.\"},\n",
        "\n",
        "    {\"prompt\": \"Electric vehicles are gaining popularity because\",\n",
        "     \"reference\": \"they reduce dependence on fossil fuels.\"},\n",
        "\n",
        "    {\"prompt\": \"A good user interface improves usability by\",\n",
        "     \"reference\": \"making applications easier to navigate.\"},\n",
        "\n",
        "    {\"prompt\": \"Cloud services are preferred since\",\n",
        "     \"reference\": \"they provide scalable computing resources.\"},\n",
        "\n",
        "    {\"prompt\": \"Digital payments are increasing as\",\n",
        "     \"reference\": \"people prefer cashless transactions.\"},\n",
        "\n",
        "    {\"prompt\": \"Good leadership is important because\",\n",
        "     \"reference\": \"it guides teams toward shared goals.\"},\n",
        "\n",
        "    {\"prompt\": \"E-commerce platforms succeed when\",\n",
        "     \"reference\": \"they offer convenience and reliability.\"},\n",
        "\n",
        "    {\"prompt\": \"System failures can be costly if\",\n",
        "     \"reference\": \"critical services are interrupted.\"},\n",
        "\n",
        "    {\"prompt\": \"Mental health awareness has increased as\",\n",
        "     \"reference\": \"society recognizes emotional well-being.\"},\n",
        "\n",
        "    {\"prompt\": \"Renewable energy adoption grows when\",\n",
        "     \"reference\": \"environmental concerns become urgent.\"},\n",
        "\n",
        "    {\"prompt\": \"Problem solving skills improve when\",\n",
        "     \"reference\": \"individuals practice logical thinking.\"},\n",
        "\n",
        "    {\"prompt\": \"Network latency becomes noticeable if\",\n",
        "     \"reference\": \"data transmission is delayed.\"},\n",
        "\n",
        "    {\"prompt\": \"Security updates are essential because\",\n",
        "     \"reference\": \"they protect systems from vulnerabilities.\"},\n",
        "\n",
        "    {\"prompt\": \"Team productivity increases when\",\n",
        "     \"reference\": \"roles and responsibilities are clear.\"},\n",
        "\n",
        "    {\"prompt\": \"Mobile applications succeed when\",\n",
        "     \"reference\": \"they deliver a smooth user experience.\"},\n",
        "\n",
        "    {\"prompt\": \"Technological innovation accelerates when\",\n",
        "     \"reference\": \"research and development are well funded.\"}\n",
        "]\n",
        "\n",
        "# models compared\n",
        "model_names = [\n",
        "    \"gpt2\",\n",
        "    \"distilgpt2\",\n",
        "    \"facebook/opt-125m\",\n",
        "    \"EleutherAI/gpt-neo-125m\",\n",
        "    \"microsoft/DialoGPT-small\"\n",
        "]\n",
        "\n",
        "# parameters for topsis (asked from chatgpt)\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "param_cols = [\n",
        "    \"BLEU\",\n",
        "    \"ROUGE-L\",\n",
        "    \"METEOR\",\n",
        "    \"Perplexity\",\n",
        "    \"Inference Time (s)\"\n",
        "]\n",
        "\n",
        "impacts = [\"+\", \"+\", \"+\", \"-\", \"-\"]\n",
        "weights = [0.2] * 5\n",
        "\n",
        "# evaluation\n",
        "granular_data = []\n",
        "\n",
        "for model_id in model_names:\n",
        "    print(f\"Processing model: {model_id}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        inputs = tokenizer(item[\"prompt\"], return_tensors=\"pt\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=20,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "        inf_time = time.time() - start_time\n",
        "\n",
        "        gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        gen_text = gen_text.replace(item[\"prompt\"], \"\").strip()\n",
        "\n",
        "        b_score = bleu.compute(predictions=[gen_text], references=[item[\"reference\"]])[\"bleu\"]\n",
        "        r_score = rouge.compute(predictions=[gen_text], references=[item[\"reference\"]])[\"rougeL\"]\n",
        "        m_score = meteor.compute(predictions=[gen_text], references=[item[\"reference\"]])[\"meteor\"]\n",
        "\n",
        "        ref_inputs = tokenizer(item[\"reference\"], return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            loss = model(ref_inputs[\"input_ids\"], labels=ref_inputs[\"input_ids\"]).loss\n",
        "        p_score = torch.exp(loss).item()\n",
        "\n",
        "        granular_data.append({\n",
        "            \"Prompt_ID\": i + 1,\n",
        "            \"Model\": model_id,\n",
        "            \"BLEU\": b_score,\n",
        "            \"ROUGE-L\": r_score,\n",
        "            \"METEOR\": m_score,\n",
        "            \"Perplexity\": p_score,\n",
        "            \"Inference Time (s)\": inf_time\n",
        "        })\n",
        "\n",
        "df_granular = pd.DataFrame(granular_data)\n",
        "\n",
        "# topsis function\n",
        "def run_topsis(df_input, cols, impacts, weights):\n",
        "    df_norm = df_input.copy()\n",
        "\n",
        "    for i, col in enumerate(cols):\n",
        "        denom = np.sqrt((df_input[col] ** 2).sum())\n",
        "\n",
        "        if denom == 0:\n",
        "            df_norm[col] = 0\n",
        "        else:\n",
        "            df_norm[col] = (df_input[col] / denom) * weights[i]\n",
        "\n",
        "    ideal_best = []\n",
        "    ideal_worst = []\n",
        "\n",
        "    for i, col in enumerate(cols):\n",
        "        if impacts[i] == \"+\":\n",
        "            ideal_best.append(df_norm[col].max())\n",
        "            ideal_worst.append(df_norm[col].min())\n",
        "        else:\n",
        "            ideal_best.append(df_norm[col].min())\n",
        "            ideal_worst.append(df_norm[col].max())\n",
        "\n",
        "    S_plus = []\n",
        "    S_minus = []\n",
        "\n",
        "    for _, row in df_norm.iterrows():\n",
        "        d_pos = 0\n",
        "        d_neg = 0\n",
        "        for i, col in enumerate(cols):\n",
        "            d_pos += (row[col] - ideal_best[i]) ** 2\n",
        "            d_neg += (row[col] - ideal_worst[i]) ** 2\n",
        "        S_plus.append(np.sqrt(d_pos))\n",
        "        S_minus.append(np.sqrt(d_neg))\n",
        "\n",
        "    S_plus = np.array(S_plus)\n",
        "    S_minus = np.array(S_minus)\n",
        "\n",
        "    return S_minus / (S_plus + S_minus + 1e-9)\n",
        "\n",
        "# graph 1 - prompt-wise best model\n",
        "winners = []\n",
        "\n",
        "for pid in df_granular[\"Prompt_ID\"].unique():\n",
        "    slice_df = df_granular[df_granular[\"Prompt_ID\"] == pid].copy()\n",
        "    slice_df[param_cols] = slice_df[param_cols].fillna(0)\n",
        "    slice_df[\"Score\"] = run_topsis(slice_df, param_cols, impacts, weights)\n",
        "    best_row = slice_df.loc[slice_df[\"Score\"].idxmax()]\n",
        "    winners.append({\"Prompt_ID\": pid, \"Best Model\": best_row[\"Model\"]})\n",
        "\n",
        "df_winners = pd.DataFrame(winners)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "model_map = {m: i for i, m in enumerate(model_names)}\n",
        "plt.scatter(\n",
        "    df_winners[\"Prompt_ID\"],\n",
        "    [model_map[m] for m in df_winners[\"Best Model\"]],\n",
        "    s=200,\n",
        "    edgecolors=\"black\"\n",
        ")\n",
        "plt.yticks(range(len(model_names)), model_names)\n",
        "plt.xlabel(\"Prompt ID\")\n",
        "plt.ylabel(\"Best Model\")\n",
        "plt.title(\"Best Model for Each Prompt (Prompt-wise TOPSIS)\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# graph 2 - topsis ranking\n",
        "df_avg = df_granular.groupby(\"Model\")[param_cols].mean().reset_index()\n",
        "df_avg[param_cols] = df_avg[param_cols].fillna(0)\n",
        "df_avg[\"TOPSIS Score\"] = run_topsis(df_avg, param_cols, impacts, weights)\n",
        "df_avg = df_avg.sort_values(\"TOPSIS Score\", ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(df_avg[\"Model\"], df_avg[\"TOPSIS Score\"], edgecolor=\"black\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"TOPSIS Score\")\n",
        "plt.title(\"Overall Model Ranking using TOPSIS\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# final topsis table\n",
        "df_avg\n"
      ],
      "metadata": {
        "id": "RJgJPvyi8Nmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_avg = df_avg.sort_values(by=\"TOPSIS Score\", ascending=False)\n",
        "df_avg[\"Rank\"] = range(1, len(df_avg) + 1)\n",
        "df_avg = df_avg.reset_index(drop=True)\n",
        "\n",
        "df_avg"
      ],
      "metadata": {
        "id": "tLVLqGpatAhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
